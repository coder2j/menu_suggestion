{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "350e04c5-f589-44d0-9682-200006c656da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dba8129f-5797-4bff-b1d5-49b1ada1c666",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to ../data/...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('words', download_dir='../data/')\n",
    "words = set(nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd873f02-81f6-4f2a-b167-4ddcc5ae4a0b",
   "metadata": {},
   "source": [
    "### Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "698eb9c0-78b7-4132-ab92-dd28ec5ea8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 43440\n",
      "drwxr-xr-x   4 huangjunming  staff       128 May 24 14:05 \u001b[34m.\u001b[m\u001b[m\n",
      "drwxr-xr-x  11 huangjunming  staff       352 May 25 15:47 \u001b[34m..\u001b[m\u001b[m\n",
      "drwxr-xr-x   4 huangjunming  staff       128 May 24 14:05 \u001b[34mcorpora\u001b[m\u001b[m\n",
      "-rw-r--r--   1 huangjunming  staff  22237983 May 20 21:59 products.parquet.gz\n"
     ]
    }
   ],
   "source": [
    "! ls -la ../data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81bcb2c8-f4c6-4159-9391-71126f2012bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"../data/products.parquet.gz\", engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b233d91a-147d-4b72-a4b5-8a72ba77bed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>vendor_geohash</th>\n",
       "      <th>product_name</th>\n",
       "      <th>product_description</th>\n",
       "      <th>order_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>325178</th>\n",
       "      <td>325178</td>\n",
       "      <td>1247</td>\n",
       "      <td>w21zg1k</td>\n",
       "      <td>Takoyaki with Cheese</td>\n",
       "      <td>Octopus Balls with Cheese</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419213</th>\n",
       "      <td>419213</td>\n",
       "      <td>4800</td>\n",
       "      <td>w21z9kq</td>\n",
       "      <td>Poached You Mai Vegetable</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595633</th>\n",
       "      <td>595633</td>\n",
       "      <td>383</td>\n",
       "      <td>w21zerf</td>\n",
       "      <td>Kotex Slim Overnight Wing Sanitary Pad - 35cm ...</td>\n",
       "      <td></td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506708</th>\n",
       "      <td>506708</td>\n",
       "      <td>11007</td>\n",
       "      <td>w21zmp0</td>\n",
       "      <td>Mutton Briyani</td>\n",
       "      <td>None</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256534</th>\n",
       "      <td>256534</td>\n",
       "      <td>912</td>\n",
       "      <td>w23b1s5</td>\n",
       "      <td>Double Filet-O-Fish® Upsized Meal</td>\n",
       "      <td>For serious fish lovers. That's two white-fish...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        product_id  vendor_id vendor_geohash  \\\n",
       "325178      325178       1247        w21zg1k   \n",
       "419213      419213       4800        w21z9kq   \n",
       "595633      595633        383        w21zerf   \n",
       "506708      506708      11007        w21zmp0   \n",
       "256534      256534        912        w23b1s5   \n",
       "\n",
       "                                             product_name  \\\n",
       "325178                               Takoyaki with Cheese   \n",
       "419213                          Poached You Mai Vegetable   \n",
       "595633  Kotex Slim Overnight Wing Sanitary Pad - 35cm ...   \n",
       "506708                                    Mutton Briyani    \n",
       "256534                  Double Filet-O-Fish® Upsized Meal   \n",
       "\n",
       "                                      product_description  order_count  \n",
       "325178                          Octopus Balls with Cheese            2  \n",
       "419213                                               None            4  \n",
       "595633                                                              14  \n",
       "506708                                               None            7  \n",
       "256534  For serious fish lovers. That's two white-fish...            2  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afbc49b2-10db-47cc-9680-f60aa9db8198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(633148, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd840fac-2149-417c-b0b7-e67b36b38fd9",
   "metadata": {},
   "source": [
    "### Remove non English text in product name and description and non-valid items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "360d0fe1-0b0e-46ed-a498-194df1dc69fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_english(in_text):\n",
    "    if in_text is None:\n",
    "        return None\n",
    "    else:\n",
    "        return ' '.join([w for w in nltk.wordpunct_tokenize(in_text) \n",
    "                         if w.lower() in words or w.encode().isalpha()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db4609d0-1028-4b54-8ea6-ecc32fa4d66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non english text\n",
    "df['product_name'] = df['product_name'].apply(lambda x: remove_non_english(x))\n",
    "df['product_description'] = df['product_description'].apply(lambda x: remove_non_english(x))\n",
    "\n",
    "# remove rows without product name \n",
    "df = df.loc[df['product_name'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "695a9c64-2655-4223-80a6-5397bfb86e58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>vendor_geohash</th>\n",
       "      <th>product_name</th>\n",
       "      <th>product_description</th>\n",
       "      <th>order_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>622967</th>\n",
       "      <td>622967</td>\n",
       "      <td>6098</td>\n",
       "      <td>w21ze4y</td>\n",
       "      <td>Plain Prata</td>\n",
       "      <td>None</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196798</th>\n",
       "      <td>196798</td>\n",
       "      <td>6780</td>\n",
       "      <td>w21zbg7</td>\n",
       "      <td>Masala Tea</td>\n",
       "      <td>Hot</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392118</th>\n",
       "      <td>392118</td>\n",
       "      <td>9074</td>\n",
       "      <td>w21z7k9</td>\n",
       "      <td>Mini Strudel</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79466</th>\n",
       "      <td>79466</td>\n",
       "      <td>6372</td>\n",
       "      <td>w23b673</td>\n",
       "      <td>DODO YTF Sambal Chilli Paste Chilled g</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214941</th>\n",
       "      <td>214941</td>\n",
       "      <td>9332</td>\n",
       "      <td>w21z64s</td>\n",
       "      <td>Melvados Hot Spicy Tortilla Chips g</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        product_id  vendor_id vendor_geohash  \\\n",
       "622967      622967       6098        w21ze4y   \n",
       "196798      196798       6780        w21zbg7   \n",
       "392118      392118       9074        w21z7k9   \n",
       "79466        79466       6372        w23b673   \n",
       "214941      214941       9332        w21z64s   \n",
       "\n",
       "                                  product_name product_description  \\\n",
       "622967                             Plain Prata                None   \n",
       "196798                              Masala Tea                 Hot   \n",
       "392118                            Mini Strudel                None   \n",
       "79466   DODO YTF Sambal Chilli Paste Chilled g                None   \n",
       "214941     Melvados Hot Spicy Tortilla Chips g                None   \n",
       "\n",
       "        order_count  \n",
       "622967           20  \n",
       "196798            1  \n",
       "392118            3  \n",
       "79466             1  \n",
       "214941            1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2132f22f-a044-4b1f-9717-ecd82c1a3b3d",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88cfb761-c419-4616-9fcb-cd104ca07b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are in total 632825 transactions with 115685 unique menus and 13312 unique vendors in 2587 differenct locations with monthly orders range from 1 to 1386\n"
     ]
    }
   ],
   "source": [
    "n_rows = df.shape[0]\n",
    "n_product = df['product_name'].nunique()\n",
    "n_location = df['vendor_geohash'].nunique()\n",
    "n_vendor = df['vendor_id'].nunique()\n",
    "min_orders, max_orders = min(df['order_count']), max(df['order_count'])\n",
    "print(f\"There are in total {n_rows} transactions with \"\n",
    "      f\"{n_product} unique menus and {n_vendor} unique vendors \" \n",
    "      f\"in {n_location} differenct locations with monthly \"\n",
    "      f\"orders range from {min_orders} to {max_orders}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb6183e-51ee-4c3c-b742-6694f32effca",
   "metadata": {},
   "source": [
    "## Problem Definitions\n",
    "The problem can be defined as given a food delivery transaction dataset, providing a vendor with new menu suggestions which has the highest sales potential. For example, vendor A is similar as vendor B (e.g. they are both Chinese restaurant), a menu has high orders sold by vendor B has the very high potential for vendor A, a good menu suggestion can be provided to vendor A.\n",
    "\n",
    "It is quite similar to the classic movie recommandation problem. The vendor_id is the user_id, menu_id is the movie_id, the monthly orders is the rating that vendor made for the menu. Therefore, it can be regarded as a ml recomandation task. We can use collaborating filtering algorithm to solve this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb544ef-d004-46cb-ba7f-2e7a236a66fb",
   "metadata": {},
   "source": [
    "### Menu Recommandation Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "def817ee-eed9-41c1-a29a-0149d749da11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from surprise import accuracy\n",
    "from surprise import AlgoBase, CoClustering, KNNBasic, KNNWithMeans, KNNWithZScore, NMF, SVD\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2cd473-3cdf-45a4-9393-123a1d2637e6",
   "metadata": {},
   "source": [
    "#### Utility functions for menu suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "504be1c2-0566-4e5e-8c11-4de68a2ae022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_menus(meta_df: pd.DataFrame, vendor_id: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "        Return all the menus that are not exist on current vendor\n",
    "    \"\"\"\n",
    "    vendor_mask = meta_df['vendor_id'] == vendor_id\n",
    "    exist_menus = meta_df.loc[vendor_mask, 'menu_id'].unique()\n",
    "    all_menus = meta_df['menu_id'].unique()\n",
    "    return all_menus[~np.isin(all_menus, exist_menus)]\n",
    "\n",
    "def make_menu_suggestions(model: AlgoBase, \n",
    "                          vendor_id: int, \n",
    "                          meta_df: pd.DataFrame, \n",
    "                          menu_meta: pd.DataFrame, \n",
    "                          n: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "        Make n menu suggestions which has the top orders potential\n",
    "    \"\"\"\n",
    "    test_set = [(vendor_id, menu_id, 0.0) for menu_id in get_new_menus(meta_df, vendor_id)]\n",
    "    predictions = pd.DataFrame(model.test(test_set)).sort_values(by=['est'], ascending=False)\n",
    "    suggestions = predictions.set_index('iid')[['est']].join(menu_meta).head(n)\n",
    "    print(f\"The top {n} new menu suggestions for vendor {vendor_id} are as followed:\")\n",
    "    for ind, (indx, row) in enumerate(suggestions.iterrows()):\n",
    "        print(f\"{ind+1}. menu id: {indx}, estimated monthly orders: {np.ceil(row['est'])}, menu name: {row['product_name']}.\")\n",
    "    return suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccbb8ae-f96f-495b-9101-a2346bffb584",
   "metadata": {},
   "source": [
    "#### Model related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "411c00cc-a477-4da5-9b50-dbc3741558f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models(model_name: str) -> AlgoBase:\n",
    "    \"\"\"\n",
    "        Return model object from model name\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        \"CoClustering\": CoClustering(),\n",
    "        \"SVD\": SVD(),\n",
    "        \"KNNBasic\": KNNBasic(),\n",
    "        \"KNNWithMeans\": KNNWithMeans(),\n",
    "        \"KNNWithZScore\": KNNWithZScore(),\n",
    "        \"NMF\": NMF()\n",
    "    }\n",
    "    return models[model_name]\n",
    "\n",
    "def model_selection(models: List[str], \n",
    "                    data: Dataset, \n",
    "                    n_folds: int) -> (str, float, dict):\n",
    "    \"\"\"\n",
    "        Select the best model based on the cross validation experiment errors\n",
    "    \"\"\"\n",
    "    errors = {model: cross_validation(model, data, n_folds) for model in models}\n",
    "    best_model = min(errors, key=errors.get)\n",
    "    best_model_error = errors[best_model]\n",
    "    print(f\"Best model is {best_model} with cv error of {best_model_error}\")\n",
    "    return best_model, best_model_error, errors\n",
    "\n",
    "def cross_validation(model_name: str, \n",
    "                    data: Dataset, \n",
    "                    n_folds: int) -> float:\n",
    "    \"\"\"\n",
    "        Cross validation experiment for the given model and dataset\n",
    "    \"\"\"\n",
    "    print(f\"Processing cv for model {model_name} with {n_folds} folds.\")\n",
    "    kf = KFold(n_splits=n_folds)\n",
    "    algo = get_models(model_name)\n",
    "    errors = []\n",
    "    for indx, (trainset, testset) in enumerate(kf.split(data)):\n",
    "        print(f\"Processing fold {indx+1}\")\n",
    "        # train and test algorithm.\n",
    "        algo.fit(trainset)\n",
    "        predictions = algo.test(testset)\n",
    "\n",
    "        # Compute and print Root Mean Squared Error\n",
    "        errors.append(accuracy.rmse(predictions, verbose=True))\n",
    "    return np.array(errors).mean()\n",
    "\n",
    "def retrain_model(model_name: str, data: Dataset) -> AlgoBase:\n",
    "    \"\"\"\n",
    "        Retrain the model with the full data as trainset\n",
    "    \"\"\"\n",
    "    algo = get_models(model_name)\n",
    "    trainset = data.build_full_trainset()\n",
    "    algo.fit(trainset)\n",
    "    return algo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf771d0-6b6c-445f-b96e-6197b5f49002",
   "metadata": {},
   "source": [
    "#### Data preprocessing for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9ee5969-0bac-431d-9bce-0496c8e44bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give menu id to each unique product name\n",
    "df = df.assign(menu_id=df.product_name.astype('category').cat.codes)\n",
    "\n",
    "# Get the map between menu id and product name\n",
    "menu_meta = df.groupby(by=['menu_id'])[['product_name']].first()\n",
    "\n",
    "# prepare the dataframe for the recommandation model\n",
    "meta_df = df[['vendor_id', 'menu_id', 'order_count']]\n",
    "\n",
    "min_orders, max_orders = min(df['order_count']), max(df['order_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "839ddf5a-52ac-4e04-886f-823309f27fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A reader is still needed but only the rating_scale param is requiered.\n",
    "reader = Reader(rating_scale=(min_orders, max_orders))\n",
    "\n",
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "data = Dataset.load_from_df(meta_df, reader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48d54c32-e374-48a5-8674-aca6725b317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = ['SVD', 'CoClustering', 'KNNBasic', 'KNNWithMeans', 'KNNWithZScore', 'NMF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2be1b8bf-6f35-42aa-a069-3caf02408587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing cv for model SVD with 3 folds.\n",
      "Processing fold 1\n",
      "RMSE: 1345.1261\n",
      "Processing fold 2\n",
      "RMSE: 1344.9618\n",
      "Processing fold 3\n",
      "RMSE: 1343.8459\n",
      "Processing cv for model CoClustering with 3 folds.\n",
      "Processing fold 1\n",
      "RMSE: 17.8695\n",
      "Processing fold 2\n",
      "RMSE: 18.4623\n",
      "Processing fold 3\n",
      "RMSE: 18.8413\n",
      "Processing cv for model KNNBasic with 3 folds.\n",
      "Processing fold 1\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 18.7267\n",
      "Processing fold 2\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 17.7277\n",
      "Processing fold 3\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 18.0003\n",
      "Processing cv for model KNNWithMeans with 3 folds.\n",
      "Processing fold 1\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 16.3905\n",
      "Processing fold 2\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 17.5636\n",
      "Processing fold 3\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 17.1981\n",
      "Processing cv for model KNNWithZScore with 3 folds.\n",
      "Processing fold 1\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 17.1365\n",
      "Processing fold 2\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 16.8887\n",
      "Processing fold 3\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 16.7901\n",
      "Processing cv for model NMF with 3 folds.\n",
      "Processing fold 1\n",
      "RMSE: 20.9791\n",
      "Processing fold 2\n",
      "RMSE: 20.5652\n",
      "Processing fold 3\n",
      "RMSE: 20.6439\n",
      "Best model is KNNWithZScore with cv error of 16.93842541811326\n"
     ]
    }
   ],
   "source": [
    "# Select the best model based on the cross validation experiment\n",
    "best_m, best_e, errors = model_selection(models_list, data, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c00f169-8896-4ee7-a970-295faa064c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    }
   ],
   "source": [
    "# Retrain the best model with all the data as trainset\n",
    "model = retrain_model(best_m, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12aa5e8-c759-4cd6-96e9-25aeb5b94bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a71f475-f907-4ac8-af31-ebeb1dfd3e68",
   "metadata": {},
   "source": [
    "### Make some suggestions with the best model and see the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37aa11b2-2f9c-432c-87ad-cbadc5d1ee1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menu that vendor 1164 is selling: ['Shanghai Fried Rice With Grilled Chicken'\n",
      " 'Egg Fried Rice With Grilled Chicken' 'Fresh Ramen With Seasoned Prawns'\n",
      " 'Egg Fried Rice With Braised Beef' 'Fresh Ramen With Grilled Chicken'\n",
      " 'Shanghai Fried Rice With Braised Beef' 'Egg Fried Rice'\n",
      " 'Fresh Udon With Grilled Chicken' 'Egg Fried Rice With Seasoned Prawns'\n",
      " 'Truffle Fried Rice MAX' 'Truffle Fried Rice With Smoked Turkey Pastrami'\n",
      " 'Truffle Fried Rice With Braised Beef' 'Truffle Fried Rice'\n",
      " 'Egg Fried Rice HE With Braised Beef' 'Truffle Udon With Grilled Chicken'\n",
      " 'Egg Fried Rice HE' 'Shanghai Fried Rice HE With Braised Beef'\n",
      " 'Truffle Udon With Smoked Turkey Pastrami'\n",
      " 'Shanghai Fried Rice HE With Grilled Chicken'\n",
      " 'Fresh Ramen HE With Seasoned Prawns'\n",
      " 'Truffle Fried Rice With Seasoned Prawns'\n",
      " 'Fresh Udon HE With Seasoned Prawns'\n",
      " 'Fresh Ramen With Smoked Turkey Pastrami'\n",
      " 'Fresh Udon With Smoked Turkey Pastrami'\n",
      " 'Egg Fried Rice With Smoked Turkey Pastrami'\n",
      " 'Truffle Fried Rice With Grilled Chicken'\n",
      " 'Egg Fried Rice HE With Seasoned Prawns' 'Fresh Udon'\n",
      " 'Shanghai Fried Rice With Smoked Turkey Pastrami'\n",
      " 'Fresh Udon With Braised Beef' 'Fresh Ramen' 'Shanghai Fried Rice'\n",
      " 'Shanghai Fried Rice With Seasoned Prawns'\n",
      " 'Fresh Ramen With Braised Beef' 'Fresh Udon With Seasoned Prawns']\n",
      "The top 3 new menu suggestions for vendor 1164 are as followed:\n",
      "1. menu id: 37116, estimated monthly orders: 189.0, menu name: Fried Hokkien Mee.\n",
      "2. menu id: 60883, estimated monthly orders: 151.0, menu name: Malabar Chicken Dum Biryani.\n",
      "3. menu id: 75771, estimated monthly orders: 146.0, menu name: Penang Fried Kway Teow.\n"
     ]
    }
   ],
   "source": [
    "# 1164 is a vendor who sells asia food, the menus suggested are similar asia food with the best sales potential\n",
    "vendor = 1164\n",
    "menus_exist = df.loc[df['vendor_id'] == vendor, 'product_name'].unique()\n",
    "print(f\"Menu that vendor {vendor} is selling: {menus_exist}\")\n",
    "suggests = make_menu_suggestions(model, vendor, meta_df, menu_meta, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5c9be67-019e-453b-ad6d-44d1fa471920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menu that vendor 58 is selling: ['Pancakes' 'Chicago Cheesecake' 'Eggs Ben' 'Red Velvet Sliced Cake'\n",
      " 'Chocolate of a Thousand Leaves' 'Red Velvet Ice Blended'\n",
      " 'Pandan Gula Melaka Cake' 'The Original Mocha Ice Blended'\n",
      " 'New York Chicken Sausage Puff' 'Pure Dark Chocolate Ice Blended'\n",
      " 'Skinny Triple Chocolate Muffin' 'Red Velvet Hot Cocoa'\n",
      " 'Speculoos Ice Blended' 'Inches Tortilla Pizza'\n",
      " 'Belgian Dark Chocolate Layered Cake' 'Hazelnut Americano' 'Double Latte'\n",
      " 'Classic Green' 'Herbal Infusion Tea' 'Fruit Swedish Berries'\n",
      " 'Moroccan Mint Latte' 'Italian Espresso Capsule' 'Food for Thought'\n",
      " 'African Sunrise Tea Latte' 'Swedish Berries Tin Loose Leaf'\n",
      " 'Earl Grey Tea Latte' 'Red Velvet Cake' 'Special Dutch Chocolate Powder'\n",
      " 'Today s Brew Coffee' 'Sparkling Cold Brew Lemon'\n",
      " 'Buster s Weekly Cheesecake' 'Earl Grey Tin' 'Caramel Latte'\n",
      " 'Sparkling Cold Brew Passion Fruit' 'Cold Brew Vanilla Bean Latte'\n",
      " 'Cold Brew Passion Fruit' 'Cold Brew' 'Americano'\n",
      " 'English Breakfast Tea Latte' 'Classic Black Tea'\n",
      " 'Cold Brew Swedish Berries' 'Vanilla' 'The Original Vanilla Ice Blended'\n",
      " 'Caramel Macchiato' 'Cold Brew Mint Lime' 'Chai Tea Latte' 'Mocha Latte'\n",
      " 'Hazelnut Latte' 'Vanilla Latte' 'Signature Chocolate'\n",
      " 'Matcha Green Tea Latte' 'Almond Blueberry' 'Malibu Dream'\n",
      " 'Matcha Green Tea Ice Blended' 'California Sunrise' 'Caramel Ice Blended'\n",
      " 'Salmon Scramble' 'Hazelnut Ice Blended' 'Dark Chocolate Ice Blended'\n",
      " 'Pure Vanilla Ice Blended' 'Pure Caramel Ice Blended'\n",
      " 'The Ultimate Mocha Ice Blended' 'Egg Club Sandwich' 'Tortilla Pizza for'\n",
      " 'Smoked Salmon' 'Prawn Spaghetti' 'Salmon Brioche Eggs Ben'\n",
      " 'Blueberry Speculoos Muffin' 'Grandpa Mel s Tuna Puff'\n",
      " 'Spicy Tuna Spaghetti' 'Peppermint Mocha Latte' 'Tuna Mayonnaise'\n",
      " 'Mini Raspberry Yogurt Lychee Log Cake' 'Mini Classic Log Cake'\n",
      " 'Choc Cookie Crumble Ice Blended' 'Latte' 'Brek O Day'\n",
      " 'Honey Mustard Chicken' 'Tiramisu' 'Breakfast Platter'\n",
      " 'Chicken and Mushroom Spaghetti' 'Minty Chocolate Chips Cake'\n",
      " 'Spicy Chicken Puff' 'Chicken Mayonnaise' 'Peppermint Mocha Ice Blended'\n",
      " 'Turkey Ham Cheese Croissant' 'Tomato and Herb Spaghetti']\n",
      "The top 3 new menu suggestions for vendor 58 are as followed:\n",
      "1. menu id: 35507, estimated monthly orders: 210.0, menu name: Fresh Banana.\n",
      "2. menu id: 17161, estimated monthly orders: 131.0, menu name: Chew s Fresh Brown Eggs with Vitamin E x.\n",
      "3. menu id: 63453, estimated monthly orders: 122.0, menu name: Meiji Fresh Milk.\n"
     ]
    }
   ],
   "source": [
    "# 58 is a cafe, who provides coffee, breakfast and deserts. The menus suggested are similar food with the best sales potential\n",
    "vendor = 58\n",
    "menus_exist = df.loc[df['vendor_id'] == vendor, 'product_name'].unique()\n",
    "print(f\"Menu that vendor {vendor} is selling: {menus_exist}\")\n",
    "suggests = make_menu_suggestions(model, vendor, meta_df, menu_meta, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da79ef3b-5ac0-45ee-9ad2-0e8e858208df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "464117b4-feae-41b6-b612-d492bea93d2d",
   "metadata": {},
   "source": [
    "### Future Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce6ac7f-030b-46fd-824d-9eef8d7e69a7",
   "metadata": {},
   "source": [
    "If I was given more time, I can try the following steps to improve the accuray of the solution further\n",
    "1. For the current model selection, there is no hyper-parameter tuning steps. For a further improvement, conducting a grid search or random search of the hyper-parameters (various similarity measurement, error metrics, and so on).\n",
    "\n",
    "2. More features can be included. The location of vendor can be also important to measure vendor similarity.\n",
    "\n",
    "3. More data. To increase the scale of the dataset can reduce the sparcity of the data so that having better recommandations.\n",
    "\n",
    "4. More complicated models (state of the arts)\n",
    "\n",
    "5. Instead of collaborating filtering, we can also try the content based recommandation algorithm. In this case, we can use the product name and description to measure the similarity of the menu. And make menu recommandation which is very similiar to the best-seller menu that a vendor currently has.\n",
    "\n",
    "6. More granular data cleaning. We can keep the menu meta info like the weights, how many stueck, and etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0947b86e-e484-40f8-8dd3-2602cf3c05aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
